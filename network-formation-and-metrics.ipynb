{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['Source', 'Target', 'Module', 'Blame_Type', 'Ideology'], dtype='object')\n",
      "DataFrame shape: (18732, 5)\n",
      "                Source               Target                   Module  \\\n",
      "0            256116639             68034431                      Tax   \n",
      "1            271035921           2298453350  Foreign Exchange Market   \n",
      "2            433673143  1488614157071097861  Foreign Exchange Market   \n",
      "3  1362156660677083145             68034431      Retirement Pensions   \n",
      "4             96767975  1126745319901765633     Inflation and Prices   \n",
      "\n",
      "                 Blame_Type             Ideology  \n",
      "0      Presidents' Failures  turkish_nationalism  \n",
      "1      Presidents' Failures  turkish_nationalism  \n",
      "2   Blaming External Actors  turkish_nationalism  \n",
      "3  Presidents' Achievements  turkish_nationalism  \n",
      "4  Presidents' Achievements           liberalism  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "input_path = \"/mnt/c/Users/Topcu/Desktop/240416_network/merged_network_predictions_users.jsonl\"\n",
    "output_csv_path = \"/mnt/c/Users/Topcu/Desktop/gephi_retweet_network_data_single_category.csv\"\n",
    "\n",
    "main_categories = {\n",
    "    'interest rates': 'Interest Rates',\n",
    "    'tax': 'Tax',\n",
    "    'foreign exchange market': 'Foreign Exchange Market',\n",
    "    'inflation and prices': 'Inflation and Prices',\n",
    "    'raise increases': 'Raise Increases',\n",
    "    'minimum wage and wages': 'Minimum Wage and Wages',\n",
    "    'retirement pensions': 'Retirement Pensions',\n",
    "    'unemployment': 'Unemployment',\n",
    "}\n",
    "\n",
    "main_categories_blame_type = {\n",
    "    \"presidents' achievements\": \"Presidents' Achievements\",\n",
    "    \"presidents' failures\": \"Presidents' Failures\",\n",
    "    'blaming external actors': 'Blaming External Actors',\n",
    "  \n",
    "}\n",
    "\n",
    "def process_entries(entries, categories):\n",
    "    if not entries:\n",
    "        return []\n",
    "    processed_entries = [categories.get(entry.strip().lower()) for entry in entries if entry.strip().lower() in categories]\n",
    "    return list(filter(None, processed_entries))\n",
    "\n",
    "def prepare_for_gephi(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet['tweet_type'] == 'retweet' and tweet['ref_user_id']:\n",
    "                module_1 = tweet.get('module_1') or []\n",
    "                module_2 = tweet.get('module_2') or []\n",
    "                \n",
    "                modules = process_entries(module_1 + module_2, main_categories)\n",
    "                modules = [m for m in modules if m != 'no category']\n",
    "                \n",
    "                if len(modules) != 1:\n",
    "                    continue\n",
    "\n",
    "                blame_types = process_entries(tweet.get('blame_type', []), main_categories_blame_type)\n",
    "                if not blame_types or len(blame_types) > 1:\n",
    "                    continue\n",
    "\n",
    "                data.append({\n",
    "                    'Source': tweet['user_id'],\n",
    "                    'Target': tweet['ref_user_id'],\n",
    "                    'Module': modules[0],\n",
    "                    'Blame_Type': \", \".join(blame_types),\n",
    "                    'Ideology': tweet.get('user_ideology', 'Unknown')\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = prepare_for_gephi(input_path)\n",
    "df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 257941\n",
      "Retweets: 95037\n",
      "With reference user ID: 94990\n",
      "With valid module: 40864\n",
      "With valid blame type: 18732\n",
      "Tweets with missing ref_user_id: 47\n",
      "Columns in DataFrame: Index(['Source', 'Target', 'Module', 'Blame_Type', 'Ideology'], dtype='object')\n",
      "DataFrame shape: (18732, 5)\n",
      "                Source               Target                   Module  \\\n",
      "0            256116639             68034431                      tax   \n",
      "1            271035921           2298453350  foreign exchange market   \n",
      "2            433673143  1488614157071097861  foreign exchange market   \n",
      "3  1362156660677083145             68034431      retirement pensions   \n",
      "4             96767975  1126745319901765633     inflation and prices   \n",
      "\n",
      "                 Blame_Type             Ideology  \n",
      "0      presidents' failures  turkish_nationalism  \n",
      "1      presidents' failures  turkish_nationalism  \n",
      "2   blaming external actors  turkish_nationalism  \n",
      "3  presidents' achievements  turkish_nationalism  \n",
      "4  presidents' achievements           liberalism  \n",
      "Number of nodes: 19708\n",
      "Number of edges: 18362\n",
      "Number of isolated nodes: 0\n",
      "Modularity: 0.9411619931275215\n",
      "Graph Density: 4.7277762306896916e-05\n",
      "Average Clustering Coefficient: 0.0010818280837285475\n",
      "Average Shortest Path Length: None\n",
      "Diameter: None\n",
      "Assortativity based on ideology: 0.025250397075706185\n",
      "Assortativity based on blame type: 0.009174652011591518\n",
      "Assortativity based on module: 0.0066386251736618475\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "\n",
    "def process_entries(entries, main_categories):\n",
    "    return [entry for entry in entries if entry in main_categories]\n",
    "\n",
    "def create_network(df):\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(str(row['Source']), str(row['Target']), module=row['Module'], blame_type=row['Blame_Type'], ideology=row['Ideology'])\n",
    "        G.nodes[str(row['Source'])]['ideology'] = row['Ideology']\n",
    "        G.nodes[str(row['Source'])]['module'] = row['Module']\n",
    "        G.nodes[str(row['Source'])]['blame_type'] = row['Blame_Type']\n",
    "    return G\n",
    "\n",
    "def detect_communities(G):\n",
    "    partition = community_louvain.best_partition(G.to_undirected())\n",
    "    return partition\n",
    "\n",
    "def create_dataframe(G, partition):\n",
    "    nx.set_node_attributes(G, partition, 'community')\n",
    "    data = [{'node': node, 'community': data['community'], 'blame_type': data.get('blame_type', 'No Blame'), 'ideology': data.get('ideology', 'Unknown')}\n",
    "            for node, data in G.nodes(data=True)]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def prepare_for_gephi(file_path):\n",
    "    total_count = 0\n",
    "    retweet_count = 0\n",
    "    ref_user_id_count = 0\n",
    "    valid_module_count = 0\n",
    "    valid_blame_type_count = 0\n",
    "    data = []\n",
    "    missing_ref_user_id = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            total_count += 1\n",
    "            tweet = json.loads(line)\n",
    "\n",
    "            if tweet['tweet_type'] == 'retweet':\n",
    "                retweet_count += 1\n",
    "                if tweet['ref_user_id']:\n",
    "                    ref_user_id_count += 1\n",
    "                    module_1 = tweet.get('module_1') or []\n",
    "                    module_2 = tweet.get('module_2') or []\n",
    "                    \n",
    "                    modules = process_entries(module_1 + module_2, main_categories)\n",
    "                    modules = [m for m in modules if m != 'no category']\n",
    "                    \n",
    "                    if len(modules) == 1:\n",
    "                        valid_module_count += 1\n",
    "                        blame_types = process_entries(tweet.get('blame_type', []), main_categories_blame_type)\n",
    "                        \n",
    "                        if not blame_types or len(blame_types) > 1:\n",
    "                            continue\n",
    "                        valid_blame_type_count += 1\n",
    "\n",
    "                        data.append({\n",
    "                            'Source': tweet['user_id'],\n",
    "                            'Target': tweet['ref_user_id'],\n",
    "                            'Module': modules[0],\n",
    "                            'Blame_Type': \", \".join(blame_types),\n",
    "                            'Ideology': tweet.get('user_ideology', 'Unknown')\n",
    "                        })\n",
    "                else:\n",
    "                    missing_ref_user_id.append(tweet)\n",
    "\n",
    "    print(f\"Total tweets: {total_count}\")\n",
    "    print(f\"Retweets: {retweet_count}\")\n",
    "    print(f\"With reference user ID: {ref_user_id_count}\")\n",
    "    print(f\"With valid module: {valid_module_count}\")\n",
    "    print(f\"With valid blame type: {valid_blame_type_count}\")\n",
    "    print(f\"Tweets with missing ref_user_id: {len(missing_ref_user_id)}\")\n",
    "\n",
    "\n",
    "    with open(\"/mnt/c/Users/Topcu/Desktop/missing_ref_user_id.json\", 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(missing_ref_user_id, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    input_path = \"/mnt/c/Users/Topcu/Desktop/240416_network/merged_network_predictions_users.jsonl\"\n",
    "    output_csv_path = \"/mnt/c/Users/Topcu/Desktop/gephi_retweet_network_data_single_category.csv\"\n",
    "\n",
    "    df = prepare_for_gephi(input_path)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"Columns in DataFrame:\", df.columns)\n",
    "    print(\"DataFrame shape:\", df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    G = create_network(df)\n",
    "\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "    isolated_nodes = [node for node in G.nodes if G.degree(node) == 0]\n",
    "    print(f\"Number of isolated nodes: {len(isolated_nodes)}\")\n",
    "\n",
    "    partition = detect_communities(G)\n",
    "    df_with_communities = create_dataframe(G, partition)\n",
    "\n",
    "    output_json_path = \"/mnt/c/Users/Topcu/Desktop/network_metrics.json\"\n",
    "    metrics = {\n",
    "        'modularity': community_louvain.modularity(partition, G.to_undirected()),\n",
    "        'graph_density': nx.density(G),\n",
    "        'number_of_nodes': G.number_of_nodes(),\n",
    "        'number_of_edges': G.number_of_edges(),\n",
    "        'average_clustering': nx.average_clustering(G.to_undirected()),\n",
    "        'average_shortest_path_length': nx.average_shortest_path_length(G.to_undirected()) if nx.is_connected(G.to_undirected()) else None,\n",
    "        'diameter': nx.diameter(G.to_undirected()) if nx.is_connected(G.to_undirected()) else None\n",
    "    }\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Modularity: {metrics['modularity']}\")\n",
    "    print(f\"Graph Density: {metrics['graph_density']}\")\n",
    "    print(f\"Average Clustering Coefficient: {metrics['average_clustering']}\")\n",
    "    print(f\"Average Shortest Path Length: {metrics['average_shortest_path_length']}\")\n",
    "    print(f\"Diameter: {metrics['diameter']}\")\n",
    "    \n",
    "    assortativity_ideology = nx.attribute_assortativity_coefficient(G, 'ideology')\n",
    "    print(f\"Assortativity based on ideology: {assortativity_ideology}\")\n",
    "    assortativity_blame_type = nx.attribute_assortativity_coefficient(G, 'blame_type')\n",
    "    print(f\"Assortativity based on blame type: {assortativity_blame_type}\")\n",
    "    assortativity_module = nx.attribute_assortativity_coefficient(G, 'module')\n",
    "    print(f\"Assortativity based on module: {assortativity_module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Sources: 15902\n",
      "Unique Targets: 4204\n",
      "Users who retweeted multiple different users: 2058\n",
      "Users who were retweeted by multiple different users: 1635\n"
     ]
    }
   ],
   "source": [
    "unique_sources = df['Source'].nunique()\n",
    "unique_targets = df['Target'].nunique()\n",
    "print(f\"Unique Sources: {unique_sources}\")\n",
    "print(f\"Unique Targets: {unique_targets}\")\n",
    "retweets_per_user = df.groupby('Source').size()\n",
    "multiple_retweets = retweets_per_user[retweets_per_user > 1].count()\n",
    "print(f\"Users who retweeted multiple different users: {multiple_retweets}\")\n",
    "retweeted_by_multiple = df.groupby('Target').size()\n",
    "multiple_retweeted = retweeted_by_multiple[retweeted_by_multiple > 1].count()\n",
    "print(f\"Users who were retweeted by multiple different users: {multiple_retweeted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
